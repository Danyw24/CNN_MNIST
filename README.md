# Reconocimiento de D√≠gitos Manuscritos mediante Redes Neuronales Profundas 
``Autor: Danyw24``

Este repositorio implementa un sistema de clasificaci√≥n de d√≠gitos manuscritos basado en redes neuronales, utilizando el conjunto de datos MNIST como referencia est√°ndar en la literatura de aprendizaje profundo. La arquitectura del modelo se fundamenta en un perceptr√≥n multicapa (MLP) con una estructura optimizada para el reconocimiento de patrones visuales, siguiendo los principios formales del aprendizaje basado en gradientes descritos en el art√≠culo **"Gradient-Based Learning Applied to Document Recognition"** de Yann LeCun et al.

## üìå **Requisitos del Entorno de Ejecuci√≥n**

Para garantizar la correcta ejecuci√≥n del modelo, se requiere el siguiente entorno de desarrollo:

- Python 3.x
- TensorFlow/Keras
- NumPy
- Matplotlib (opcional para visualizaci√≥n de resultados)

Instalaci√≥n de dependencias mediante pip:

```bash
pip install tensorflow numpy matplotlib
```

## üìÇ **Estructura Modular del Proyecto**

# üìÅ Estructura del Proyecto MNIST-CNN

```bash
‚îú‚îÄ‚îÄ dvenv/            # Entorno virtual Python
‚îú‚îÄ‚îÄ CNN_MNIST.py      # Arquitectura principal de la CNN
‚îú‚îÄ‚îÄ CNN_prediction.py # Script para hacer predicciones
‚îÇ
‚îú‚îÄ‚îÄ models/           # Directorio de modelos guardados
‚îÇ   ‚îú‚îÄ‚îÄ CNN_MNIST.keras         # Modelo completo guardado
‚îÇ   ‚îî‚îÄ‚îÄ CNN_MNIST_.weights.h5   # Pesos del modelo
‚îÇ
‚îú‚îÄ‚îÄ docs/             # Documentaci√≥n y visualizaciones
‚îÇ   ‚îú‚îÄ‚îÄ model.png            # Diagrama arquitectura del modelo
‚îÇ   ‚îú‚îÄ‚îÄ model_structure.png  # Estructura detallada en texto
‚îÇ   ‚îî‚îÄ‚îÄ Training_accuracy.png # Gr√°fico de precisi√≥n

```

## üìä **Preprocesamiento y Normalizaci√≥n de Datos**

![image](https://github.com/user-attachments/assets/2641c825-2fb9-47a5-96ed-14b328a34ea5)




### **Normalizaci√≥n de Caracter√≠sticas**

Cada imagen del conjunto MNIST presenta valores de intensidad en el rango `[0,255]`. Para mejorar la estabilidad num√©rica y la eficiencia del entrenamiento, se normalizan los valores en el intervalo `[0,1]` mediante:

```python
"""
Normalizar los valores de p√≠xeles (0-255 -> 0-1) sirve para 
que los valores de los p√≠xeles sean m√°s f√°ciles de tratar y
procesar en el modelo.  
"""
x_train = x_train / 255.0
x_test = x_test / 255.0

```


### **Transformaci√≥n Espacial**

Reformatear los datos para que la red neuronal pueda procesar los datos de entrada.
Los datos de entrada se representan como una matriz de im√°genes de 28x28 p√≠xeles en escala de grises.
y se agregan una dimensi√≥n de canal para representar cada imagen como una matriz de p√≠xeles.

```python
X = X.reshape(-1, 28, 28, 1)
```

Esta operaci√≥n preserva la informaci√≥n espacial y permite la explotaci√≥n de correlaciones estructurales en la entrada.

---

## üèóÔ∏è **Arquitectura del Modelo Neuronal**

![image](https://github.com/user-attachments/assets/498a4f6d-b61b-46af-ba14-791ceb216cd2)



La arquitectura implementada es un perceptr√≥n multicapa con una √∫nica capa oculta densa:

| Capa            | Dimensi√≥n de Entrada | N√∫mero de Par√°metros |
| --------------- | ------------------- | ------------------- |
| Flatten         | (28,28) ‚Üí 784       | 0                 |
| Dense (ReLU)    | 784 ‚Üí 128           | 100480            |
| Dense (Softmax) | 128 ‚Üí 10            | 1290              |

### **Funciones de Activaci√≥n**

- **ReLU (Rectified Linear Unit)**: Proporciona una activaci√≥n no saturante para mitigar el problema del desvanecimiento del gradiente.
- 
![image](https://github.com/user-attachments/assets/11aef82e-4310-4b88-8387-147ba95e0819)


- **Softmax**: Convierte la salida de la capa final en una distribuci√≥n de probabilidad sobre las 10 clases posibles con valores entre 0 y 1.

![image](https://github.com/user-attachments/assets/7ef664e5-71c4-48e3-b37d-b0f3a5dceb1a)

---

## ‚öôÔ∏è **Funci√≥n de P√©rdida y Algoritmo de Optimizaci√≥n**

La funci√≥n de p√©rdida empleada es **Sparse Categorical Crossentropy**, la cual mide la divergencia entre la distribuci√≥n de las predicciones y las etiquetas reales:

![image](https://github.com/user-attachments/assets/ffafe3ab-05c0-4328-b098-3eb0cd021921)

 loss: utiliza sparse_categorical_crossentropy para calcular la diferencia entre las salidas y las etiquetas
ejemplo: si la red predice [0.1, 0.8, 0.1]] y la etiqueta es [0, 1, 0] entonces el loss ser√° calculado como:
loss = tf.nn.sparse_categorical_crossentropy(y_true=[0, 1, 0], y_pred=[0.1, 0.8, 0.1]) o  L=‚àílog(0.8)
entonces **L=0.223**  


El algoritmo de optimizaci√≥n utilizado es **Adam**, que integra los m√©todos de descenso de gradiente estoc√°stico (SGD) y RMSProp para una convergencia m√°s eficiente,
La optimizaci√≥n se utiliza para ajustar los pesos del bias de par√°metros de la red neuronal, mediante
el m√©todo de descenso de gradiente y el optimizador Adam, que es una 
variante de RMSProp + SGD(descenso de gradiente estoc√°stico), RMSProp es una optimizaci√≥n que ajusta los pesos
de manera dinamica con el fin de mejorar el rendimiento del modelo en el entrenamiento.

---

## üìà **M√©tricas de Evaluaci√≥n del Modelo**

El desempe√±o del modelo se cuantifica utilizando la m√©trica de **precisi√≥n (accuracy)**, definida como:


Para evaluar el modelo entrenado:

```python
loss, acc = model.evaluate(X_test, y_test)
print(f"Precisi√≥n obtenida: {acc*100:.2f}%")
```

---

## üöÄ **Ejemplo de Implementaci√≥n del Entrenamiento**

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy

# Definici√≥n de la arquitectura del modelo
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

# Configuraci√≥n del modelo
model.compile(
    optimizer=Adam(),
    loss=SparseCategoricalCrossentropy(),
    metrics=['accuracy']
)

# Entrenamiento del modelo
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))
```

---

## üéØ **Resultados Experimentales**

![image](https://github.com/user-attachments/assets/64f4e085-c6d0-4ddf-9598-52d2f82523a2)


![image](https://github.com/user-attachments/assets/1896780b-34ec-4d8c-a8c7-63b5cdd217c3)


## üìä **M√©tricas de Rendimiento**

| M√©trica               | Entrenamiento | Validaci√≥n |
|-----------------------|---------------|------------|
| **Precisi√≥n**         | 98.86%        | 97.68%     |
| **P√©rdida**           | 0.0339        | 0.0943     |

## üöÄ **Resultado Final en Pruebas**
```python
Precisi√≥n en datos de prueba: 0.9780
```
---

## üìú **Referencias Bibliogr√°ficas**

- LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). *Gradient-Based Learning Applied to Document Recognition*. *Proceedings of the IEEE*.
- Documentaci√≥n oficial de Keras: [https://keras.io](https://keras.io)
- Recursos de TensorFlow: [https://www.tensorflow.org](https://www.tensorflow.org)

---


